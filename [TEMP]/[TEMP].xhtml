<?xml version="1.0" encoding="utf-8"?>
<!--
                                                                                     
 h       t     t                ::       /     /                     t             / 
 h       t     t                ::      //    //                     t            // 
 h     ttttt ttttt ppppp sssss         //    //  y   y       sssss ttttt         //  
 hhhh    t     t   p   p s            //    //   y   y       s       t          //   
 h  hh   t     t   ppppp sssss       //    //    yyyyy       sssss   t         //    
 h   h   t     t   p         s  ::   /     /         y  ..       s   t    ..   /     
 h   h   t     t   p     sssss  ::   /     /     yyyyy  ..   sssss   t    ..   /     
                                                                                     
	<https://y.st./>
	Copyright © 2018 Alex Yst <mailto:copyright@y.st>

	This program is free software: you can redistribute it and/or modify
	it under the terms of the GNU General Public License as published by
	the Free Software Foundation, either version 3 of the License, or
	(at your option) any later version.

	This program is distributed in the hope that it will be useful,
	but WITHOUT ANY WARRANTY; without even the implied warranty of
	MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
	GNU General Public License for more details.

	You should have received a copy of the GNU General Public License
	along with this program. If not, see <https://www.gnu.org./licenses/>.
-->
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
	<head>
		<base href="https://y.st./[TEMP]/[TEMP].xhtml"/>
		<title>Quick sort &lt;https://y.st./[TEMP]/[TEMP].xhtml&gt;</title>
		<link rel="icon" type="image/png" href="/link/CC_BY-SA_4.0/y.st./icon.png"/>
		<link rel="stylesheet" type="text/css" href="/link/main.css"/>
		<script type="text/javascript" src="/script/javascript.js"/>
		<meta name="viewport" content="width=device-width"/>
	</head>
	<body>
<nav>
</nav>
		<header>
			<h1>Quick sort</h1>
			<p>Written in <span title="Data Structures">CS 3303</span> by <a href="https://y.st./">Alexand(er|ra) Yst</a>, finalised on 2018-12-26</p>
		</header>
<section id="Unit8">
	<h2>Unit 8</h2>
	<p>
		When the textbook was talking about checking items in a sorted list to rule out many possibilities in a single comparison, I was sure it was going to discuss how we should use a binary search on sorted lists, not a basic sequential search.
		Instead, it goes on into a discussion of jump searching.
		I&apos;m not sure what advantage a jump search has over a binary search though.
		In a binary search, you can rule out about half the remaining options in a single comparison.
		Jump search doesn&apos;t have nearly that kind of effectiveness.
		Jump searching certainly does provide better efficiency than basic sequential searching though.
	</p>
	<p>
		The idea of arranging records by access probability is interesting.
		It seems a bit like a cop-out, rather than organising the records properly, but I can see why it would be beneficial for certain use cases.
		You do have to know the access probabilities to make it work though.
		The count and move-to-front methods of automated sorting have exactly the problems you&apos;d intuitively expect from them, so they&apos;re not very interesting.
		With counts, access pattern changes aren&apos;t handled well due to built-up counts from past access, and the move-to-front method pops even rarely-used records to the front that one time they actually do get used.
		For linked list implementations, the problem stops there, but for arrays, that causes a lot of value shifts.
		The transpose method caught my attention though.
		Like the book said, there&apos;s a corner case in which it performs poorly: accessing two adjacent records repeatedly will cause them to jump back and forth instead of migrating tot he front.
		Other than that though, it seems like a rather decent algorithm that balances short- and long-term access patterns well, and doesn&apos;t even require the extra overhead of storing access counts.
	</p>
	<p>
		The sections on bitmaps and hash maps were mostly review for me.
		The book&apos;s take on hash maps was a little different than what I&apos;d previously learned though.
		The version of hash mapping I&apos;d previously learned used an array of linked lists.
		First, a hash value from zero to n-1 (where n is the number of array slots) was computed to determine where the item would fall.
		Then, the item would be added to that linked list.
		Looking up a value by key then involved computing the hash again to find the right linked list, then performing a sequential search on that linked list.
		If the array size was close to the right size for the amount of data, the linked list dealt with would have somewhere from zero to two items, so the sequential search would be quick.
		This book&apos;s hash maps are different though.
		At first, I thought it was saying that a hash value is calculated and the records arranged in an array by hash value, smallest to largest.
		This allows for a binary search based on hash value to be performed.
		However, as the book continued, it turns out the book&apos;s method instead involves remapping keys to different slots when hash collisions occur, so multiple attempts still need to be made to find values in some cases.
		As expected though, either method of hash mapping does not make searches based on key ranges or partial key matches very efficient.
		All items in the hash map have to be checked for these types of searches.
		Items have an order, but they have no <strong>*meaningful*</strong> order, so checking records in order is difficult, and also requires checking all items to see which one should come next, and the same applies to finding the record with the smallest or largest key.
		Hashing works great when you just need an unordered key/value store though.
	</p>
	<p>
		Much to my surprise, once the book got done explaining hashing, it then moved on to cover both types of hashing.
		The form I was familiar with is known as open hashing, while the type the book introduced is known as closed hashing.
	</p>
	<p>
		When I read about dealing with hash collisions by choosing a new slot to put one of the items in, I just sort of assumed the new slot would be chosen similarly to how the first slot had been chosen: via some sort of hash.
		This would be pseudorandom probing.
		However, there&apos;s also another option it seems: linear probing.
		Linear probing chooses a new slot based on a very predictable sequence by adding a certain number to the home position of the item repeatedly until an empty slot is found.
		As long as the number of slots in the array cannot be evenly divided by the number chosen, just about any number can be chosen.
		One also makes a fine choice.
		Either way, each slot of the array will be tested (after wrapping around one or more times) and lead back to the home position if all slots are full.
		However, linear probing isn&apos;t a very &quot;hashy&quot; method of operation; it&apos;s basically sequential access, and not hash-calculated access.
		Hashing is useful to begin with for a reason, so intuitively, adding components to our algorithm that aren&apos;t very &quot;hashy&quot; will take away some of the benefit of hashing.
		As it turns out, linear probing caused the problem of primary clustering, which results in many values having to get checked when performing a lookup or setting a new key.
		It builds a linear search into the hash table.
		In other words, hash collisions need to be dealt with as we&apos;d expect: via pseudorandom probing.
	</p>
	<p>
		It&apos;s worth noting the possibility of quadratic probing, but it&apos;s not really a good idea because it doesn&apos;t put all slots on the probe sequence, so even with an empty slot available, it&apos;s possible no empty slot will be encountered.
		For certain table sizes though, quadratic probing using the right equation will in fact visit every slot though, and with a reduced cost over pseudorandom probing.
	</p>
	<p>
		As discussed above, long chains of full slots can occur, resulting in many data comparisons before finding the right data or a free space.
		In the case of linear probing, one of these chains causes several hash values to chain together so that the next value added on any of these hash values would all set their sights on the same empty slot.
		This is what&apos;s known as primary clustering.
		Using a better probing method eliminates that, but still has the problem of secondary clustering.
		Secondary clustering is caused when values with the same hash value follow the same probing sequence.
		This can be avoided by taking account the original key value in the probing sequence, and re-hashing it.
		This is known as double hashing.
		Again, this is just something I sort of assumed needed to be done.
		Intuitively, I wouldn&apos;t think to ignore the original key and assign the same probing sequence to every key with the same home position.
	</p>
	<p>
		The book discussed the cost of various operations, but when it mentioned deletion, I immediately thought of the impact in regards to the aforementioned hash collision resolution.
		What happens if record A is inserted, displacing the later-inserted record B that shares its home position, then record A is deleted?
		Unless the whole array gets resorted upon deletion, we can&apos;t correct for this, as there could be a chain of records in home positions in the probing sequence before we reach the displaced record B.
		The possibility of deletion seems to throw out all assumptions made about being able to stop the probing sequence once we encounter an empty slot.
		Thankfully, the next section covered this issue.
		A &quot;tombstone&quot; value is put in place of the deleted record, alerting the search code to know it needs to probe a little further.
		It also said that for insertion, the entire probing sequence must be traversed up to the point of an empty slot to verify that a duplicate key doesn&apos;t get inserted.
		I hadn&apos;t even considered that possibility.
		Tombstones of course lengthen the probing sequence needed for finding records, but periodic reorganisation of the array can fix that.
	</p>
	<p>
		I&apos;ve known about primary keys from database software such as MySQL, but for this week&apos;s reading, I learned what that actually means.
		Primary keys are unique, just like they have to be in MySQL databases, but keys in general don&apos;t have to be unique.
		I thought that if they weren&apos;t unique, they couldn&apos;t be keys at all, as data can always be looked up by key.
		Instead, data can only be looked up by primary key, but can be searched for using any key.
		Non-unique keys are known as secondary keys.
	</p>
	<p>
		ISAM seems like a rather hacky solution.
		It requires periodic updates to the databases indexes in a process easily comparable to defragmenting a hard drive.
		Like with defragmenting a hard drive, if your system is properly functional, you shouldn&apos;t need to do it.
		In the case of defragmenting, it&apos;s mostly a Windows-only problem caused by poorly-constructed filesystems (<abbr title="File Allocation Table">FAT</abbr> and <abbr title="New Technology File System">NTFS</abbr>, both of which were predictably developed by Microsoft).
		Despite countless issues such as easy corruption of data and constant fragmentation, even modern Windows uses <abbr title="New Technology File System">NTFS</abbr>.
		On a proper filesystem, such as <abbr title="fourth extended filesystem">ext4</abbr>, fragmentation is almost non-existent.
		I see how ISAM was a good start though.
		The field of database management was fairly new, so development had to begin somewhere.
		At least ISAM kept data easy to find, even if it did come at the cost of a constant need for maintenance.
	</p>
	<p>
		Binary search trees on disk seem like a pain.
		On the one hand, you can optimise them for updates, or on the other, you can optimise them for access.
		One set of operations or the other is going to be slow though.
		If you optimise for access, you end up rebalancing the tree after any update, which can involve moving a lot of nodes to other blocks on disk.
		If you optimise for insertions, you don&apos;t rebalance or rearrange the data, so looking up values can require accessing many blocks on disk.
	</p>
	<p>
		The operations of 2-3 trees are a bit difficult to picture in my mind.
		I couldn&apos;t figure out how their strange operations could keep the tree perfectly balanced until the end, when it was explained that they gain and lose levels at the root, not at the leaves.
		The amount of data in each node is variable though, which is a bit strange.
		It seems almost disorderly and wastes space in nodes that store less than the maximum amount of data, but it avoids the need to make as many changes to the tree structure as you otherwise would need to during an update.
		B-trees fall into pretty much the same boat, being a superset of 2-3 trees, and have the same advantages and drawbacks.
	</p>
	<p>
		I&apos;m not sure what the advantage of a B+-tree is.
		The textbook says it&apos;s just like a B-tree, but without usable data in the internal nodes.
		Searches in B+-trees must therefore continue until they reach the leaves, every time, while searches in regular B-trees don&apos;t have this requirement.
	</p>
	<p>
		The finals exam mostly went well.
		Each of the exams had a single question I wasn&apos;t sure what the intent was though.
		I forget the exact questions, as my exams this term are proctored, so I had to take them at the testing centre instead of at home, but there was a technically correct answer and an answer I thought the exam was likely intending.
		Some of the answer keys at this school seem to favour technically-incorrect answers based on generalisation of the topic.
		I went with the technically-correct answer in both cases, but even if they get marked wrong, I should be fine.
		There wasn&apos;t a lot of challenge to the tests.
	</p>
</section>
<section id="Unit8">
	<h2>Unit 8</h2>
	<p>
		Logic programming looks highly useful for certain types of problems, but pretty much unusable for other problems.
		Instead of issuing instructions for the computer to execute, you provide a list of relationships, and the computer goes about deducing other relationships you&apos;ve asked for based on the known relationships.
		If you wanted to solve one of those problems in which you needed to figure out what person lived in what house and had what colour of curtains, a logic programming language would allow you to write out all the known relationships and not worry about figuring out how to figure out the other relationships.
		From the sounds of it though, that simplicity for the programmer comes at a computational cost for the computer.
		The computer seems to try all possibilities; in other words, it uses brute force, which takes a lot of resources.
	</p>
	<p>
		The book makes the claim that it&apos;s difficult to imagine a language more simple than a logic programming language, but there&apos;s a reason for that.
		First, logic programming is such a high-level abstraction that most of the complexity is abstracted away.
		Second, logic programming is so high level that most of the <strong>*usefulness*</strong> is abstracted away!
		And third, it&apos;s so high level that most of the efficiency is abstracted away.
		Logic programming has a time and place, but it doesn&apos;t work for most situations.
		It&apos;s sort of like a domain-specific language in this regard; it&apos;s simpler than a language that can do anything, but it&apos;s also not able to do much (if anything) outside its domain.
		Sure, like the book says, the Horn clause is a powerful statement, but the fact that it&apos;s the only available statement (besides facts and goals) limits what you can do drastically.
	</p>
	<p>
		Using Prolog as a database is an interesting concept.
		Is it more efficient than using actual database software?
		Probably not.
		The <code>assert()</code> and <code>retract()</code> functions make it possible to update the data without editing the program itself though, so it seems feasible.
		Is there a way to save the database to disk though?
		That seems like a rather important feature, and the book didn&apos;t cover it.
	</p>
	<p>
		The description of what happens when you pass arguments of the wrong type in Prolog greatly reminded me of Java.
		It&apos;s not quite the same.
		In Prolog, it results in &quot;falure&quot;, which is logic programming&apos;s version of a boolean <code>false</code>.
		There&apos;s no good way to trace it though, and the failure has no obvious cause.
		In Java, when you pass arguments of the wrong value, the compiler complains that it can&apos;t find the method, even though the method has clearly been defined.
		Again, it&apos;s an error that doesn&apos;t appear to make any sense.
		It fails to notice that it&apos;s the argument that&apos;s the wrong type, not the method that doesn&apos;t exist, due to Java&apos;s ugly overloading feature.
	</p>
	<p>
		The finals exam mostly went well.
		Each of the exams had a single question I wasn&apos;t sure what the intent was though.
		I forget the exact questions, as my exams this term are proctored, so I had to take them at the testing centre instead of at home, but there was a technically correct answer and an answer I thought the exam was likely intending.
		Some of the answer keys at this school seem to favour technically-incorrect answers based on generalisation of the topic.
		I went with the technically-correct answer in both cases, but even if they get marked wrong, I should be fine.
		There wasn&apos;t a lot of challenge to the tests.
	</p>
</section>
		<hr/>
		<p>
			Copyright © 2018 Alex Yst;
			You may modify and/or redistribute this document under the terms of the <a rel="license" href="/license/gpl-3.0-standalone.xhtml"><abbr title="GNU&apos;s Not Unix">GNU</abbr> <abbr title="General Public License version Three or later">GPLv3+</abbr></a>.
			If for some reason you would prefer to modify and/or distribute this document under other free copyleft terms, please ask me via email.
			My address is in the source comments near the top of this document.
			This license also applies to embedded content such as images.
			For more information on that, see <a href="/en/a/licensing.xhtml">licensing</a>.
		</p>
		<p>
			<abbr title="World Wide Web Consortium">W3C</abbr> standards are important.
			This document conforms to the <a href="https://validator.w3.org./nu/?doc=https%3A%2F%2Fy.st.%2F%5BTEMP%5D%2F%5BTEMP%5D.xhtml"><abbr title="Extensible Hypertext Markup Language">XHTML</abbr> 5.2</a> specification and uses style sheets that conform to the <a href="http://jigsaw.w3.org./css-validator/validator?uri=https%3A%2F%2Fy.st.%2F%5BTEMP%5D%2F%5BTEMP%5D.xhtml"><abbr title="Cascading Style Sheets">CSS</abbr>3</a> specification.
		</p>
	</body>
</html>

