<?xml version="1.0" encoding="utf-8"?>
<!--
                                                                                     
 h       t     t                ::       /     /                     t             / 
 h       t     t                ::      //    //                     t            // 
 h     ttttt ttttt ppppp sssss         //    //  y   y       sssss ttttt         //  
 hhhh    t     t   p   p s            //    //   y   y       s       t          //   
 h  hh   t     t   ppppp sssss       //    //    yyyyy       sssss   t         //    
 h   h   t     t   p         s  ::   /     /         y  ..       s   t    ..   /     
 h   h   t     t   p     sssss  ::   /     /     yyyyy  ..   sssss   t    ..   /     
                                                                                     
	<https://y.st./>
	Copyright © 2018 Alex Yst <mailto:copyright@y.st>

	This program is free software: you can redistribute it and/or modify
	it under the terms of the GNU General Public License as published by
	the Free Software Foundation, either version 3 of the License, or
	(at your option) any later version.

	This program is distributed in the hope that it will be useful,
	but WITHOUT ANY WARRANTY; without even the implied warranty of
	MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
	GNU General Public License for more details.

	You should have received a copy of the GNU General Public License
	along with this program. If not, see <https://www.gnu.org./licenses/>.
-->
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
	<head>
		<base href="https://y.st./[TEMP]/[TEMP].xhtml"/>
		<title>Quick sort &lt;https://y.st./[TEMP]/[TEMP].xhtml&gt;</title>
		<link rel="icon" type="image/png" href="/link/CC_BY-SA_4.0/y.st./icon.png"/>
		<link rel="stylesheet" type="text/css" href="/link/main.css"/>
		<script type="text/javascript" src="/script/javascript.js"/>
		<meta name="viewport" content="width=device-width"/>
	</head>
	<body>
<nav>
</nav>
		<header>
			<h1>Quick sort</h1>
			<p>Written in <span title="Data Structures">CS 3303</span> by <a href="https://y.st./">Alexand(er|ra) Yst</a>, finalised on 2018-12-26</p>
		</header>
<section id="Unit7">
	<h2>Unit 7</h2>
	<p>
		I don&apos;t know what the first student I graded work for this week was thinking.
		They included a screenshot of their code running, but there was nothing of note in the shot.
		The code hadn&apos;t completed, so they weren&apos;t showing the output or the sorted array.
		In fact, it looked like it&apos;d been taken within seconds of the program being started.
		So to get their output so I could grade, I ran their code.
		It couldn&apos;t even complete!
		An error came up very quickly, revealing an uninstantiated variable, causing execution to halt.
		Clearly, they didn&apos;t even fully run their code to see if it could complete, let alone see if it had the right output.
		And to top it off, no description or analysis was provided.
	</p>
	<p>
		The next submission I graded looked like it was probably well-written, but they didn&apos;t include their output.
		So I tried to run their code myself so I could check the output for correctness.
		A syntax error prevents the code from compiling.
		My guess is that they developed this code outside of Jeliot.
		I don&apos;t blame them; Jeliot majorly slowed me down in developing my own solution.
		However, they should have at least tested in Jeliot before submitting, so they would know it would run for other students.
		Alternatively, they could have provided their output and I would have graded based on that.
	</p>
	<p>
		The third person again didn&apos;t provide their own output, but their code actually ran.
		It didn&apos;t output the number of swaps performed though.
	</p>
	<p>
		I was surprised to read this week that <abbr title="random-access memory">RAM</abbr> is referred to as primary memory, while disk storage is only secondary.
		As the book says, disk memory is persistent.
		In my mind, that makes it the more important memory, so it seems more worthy of being labelled as primary.
		<abbr title="random-access memory">RAM</abbr>, on the other hand, is volatile.
		It loses its contents soon after the power is cut.
		This process can be delayed (for example, by freezing the <abbr title="random-access memory">RAM</abbr>), but the data is still lost rather quickly compared to disk memory.
		<abbr title="random-access memory">RAM</abbr> is merely working memory.
		It&apos;s not the file you put in your file cabinet.
		It&apos;s the scratch paper you throw out after using it to work out your calculation.
		<abbr title="random-access memory">RAM</abbr> may be orders of magnitude faster than disk memory, but I would by no means consider it to be the main storage of the machine.
	</p>
	<p>
		The book discusses Windows versus UNIX in terms of disk layout, but I think it&apos;s missing a major detail.
		It mentions how Windows uses a file allocation table.
		That&apos;s not a property of Windows though.
		That&apos;s a property of certain disk formats, such as the File Allocation Table (<abbr title="File Allocation Table">FAT</abbr>) format.
		I don&apos;t know what format a UNIX system uses by default; I&apos;m on Linux, not actual UNIX.
		But if a UNIX system was instead using a <abbr title="File Allocation Table">FAT</abbr>-formatted disk, it too would be using a file allocation table.
	</p>
	<p>
		I knew about hard drive buffering, though it&apos;s been a while since I studied it, so it was nice to have a refresher on that.
		I don&apos;t think I knew about input and output having separate buffers though.
		It makes perfect sense why it&apos;s done that way, but isn&apos;t something I&apos;ve thought about before.
		I&apos;ve definitely not heard of having two input and two output buffers before.
		I can see why that would speed things up quite a bit.
		You can&apos;t read a buffer while it&apos;s being written to and expect consistency, so having one to read from and one to write to can at times make sure both operations are used in parallel instead of one operation waiting on the other.
	</p>
	<p>
		I was curious about how items could be sorted outside <abbr title="random-access memory">RAM</abbr>, which this week&apos;s reading assignment teased could be done.
		It turns out it can&apos;t though.
		Instead, the tactic usually used is to read part of the data needed, sort that, then write it back to disk and read the next part.
		Obviously, one item might end up moved on disk many times before it ends up in the right spot, but then again, things tend to get moved around quite a bit in <abbr title="random-access memory">RAM</abbr> during the sorting process too.
		It&apos;s not much different, aside from the fact that reading from and writing to the disk is so much slower.
		One sorting method mentioned doesn&apos;t actually sort the data at all.
		Instead, it creates a sorted index so data can be found quickly later, but the data itself is still out of order.
		However, that sorted index file is sometimes only the beginning, used to plan out how the data should be arranged.
		When this is the case, the data does in fact get sorted.
	</p>
	<p>
		The idea of using your own buffer pool system to handle large files was interesting.
		The main buffer pool may be too small, but you can definitely create your own buffer pool system external to the main buffer pool and use that to pretend you have more <abbr title="random-access memory">RAM</abbr> than you do.
		You&apos;re pretty much reinventing the wheel at that point, as the main buffer pool is already trying to make it appear like you have more <abbr title="random-access memory">RAM</abbr> than you do, but if you understand how pages and buffer pools work well enough, this is an easy solution to implement.
		Even with paging, something such as quick sort shouldn&apos;t be too hard to implement.
		Like the book said too, if you implement your own buffer system, you can better tune it to your problem as well.
		It seems the biggest time-savers though are to focus on reducing disk reads/writes and to perform activities using different components (<abbr title="random-access memory">RAM</abbr>, hard drives, processor) running in parallel.
	</p>
	<p>
		The discussion assignment for the week was a bit odd.
		We were to run code that deals with buffer pool policies, but the pools generated were so small that I&apos;m not sure they really got the point across.
		With only two pages in the buffer pool, no page-replacement policy is going to be effective without carefully-crafted inputs.
		Even having just three would be better than two, though I think you&apos;d need at least four or five for a good demonstration.
	</p>
</section>
<section id="Unit7">
	<h2>Unit 7</h2>
	<p>
		We have had no main assignments in this course.
		We&apos;ve had discussion assignments and learning journal assignments, but nothing else.
		Oh.
		Yeah.
		Also the mountainous piles of reading assignments.
		It&apos;s really weird that we&apos;ve had no main unit assignments though; this week was the last chance for any sort of assignment like that.
		If assigned next week, we wouldn&apos;t have a week after that in which to grade them, so we can&apos;t have assignments next week.
	</p>
	<p>
		Functional programming is interesting, but only from an educational standpoint.
		It doesn&apos;t seem like the sort of thing that&apos;s viable for actually writing programs.
		For example, the lack of looping structures is pretty big.
		Recursion is inefficient, so having to use it when a basic loop would do isn&apos;t good.
		Despite its inefficiency, recursion should be used when recursion is the intuitive way to express something.
		And by that same logic, loops should be used instead when loops are the intuitive way to express something, which is most of the time.
		Not having loops must hinder the readability of the programs written in the language.
	</p>
	<p>
		Lazy evaluation is just weird.
		At first, I thought it only a little strange, but completely necessary in the context of functional programming languages.
		However, once the book got to the part on tree comparisons, it was clear that there was quite a bit more oddity at work.
		In the example provided, two trees are converted into lists, then compared.
		Using lazy evaluation, the book says the trees don&apos;t even need to be fully converted if the trees are unequal, as the inequality will likely be discovered before that.
		Clearly, the process the example&apos;s code says to use is to convert the trees, then compare.
		Comparison shouldn&apos;t even begin until the full conversion of both trees is performed.
		However, the partly-formed lists are compared instead so the rest of the lists doesn&apos;t need to be computed.
		You can easily see how this would be efficient, but it&apos;s nothing short of bizarre.
	</p>
	<p>
		The list notation in Haskell is odd.
		It&apos;s pretty straightforward if you ignore the two numbers, two full stops, then a third number notation.
		Intuitively, I&apos;d have thought the first number was just a number, then the number double full stop number was a range.
		Instead though, all three are a range.
		The first two numbers are both literal numbers in the range and a declaration of the interval at which the numbers in the range are spaced, while the final number is the point at which the range stops.
		Except that that number isn&apos;t necessarily in the list, as the interval might not allow for it.
	</p>
	<p>
		The final article for the week started out by saying that programmers are procrastinators.
		The funny thing is, censorship at this school has turned me into a procrastinator.
		This censorship leaves me lethargic any time I even think about working on coursework, so I tend to put it off longer than I should.
		And when I do get going on it, I need frequent breaks or I zone out.
		And this applies to any programming assignments we have as well.
		This spreads out to procrastinating in other areas; I can&apos;t do X until I complete my coursework, so because I put my coursework off, X gets put off too.
		Under normal circumstances though, I don&apos;t procrastinate when it comes to programming.
		I tend to get started as soon as I can, because I&apos;m excited about the finished product.
		There&apos;s a lot I used to get done outside of programming too, before the censorship started.
		And between terms, I regain some of my vital energy and actually get quite a bit done.
	</p>
	<p>
		This article also asks the rhetorical question of why the universe is described using mathematics.
		Does the universe have some connection to maths?
		This question, to me, makes it seem like the author doesn&apos;t understand the root of what maths are.
		Mathematics is a system for working with data in a way that is coherent and logical.
		If something can&apos;t be described using mathematics, either you don&apos;t understand the true nature of the thing or the thing isn&apos;t coherent and consistent.
		There are far too many mathematical rules for me to cover each one, and besides, I don&apos;t claim to know them all.
		I once studied quantum mechanics for a bit, but the maths confused me and I never did get a firm grasp on them.
		But even the most simple mathematical rules are based on abstracting away fundamental principles of reality.
		Take addition, for example.
		If there&apos;s an apple on the table and you set two more apples on the table, the number of apples now on the table can be described using addition.
		Subtraction works the same way.
		As humans, we have a tendency to abstract things many times a day and not even notice.
		Currency is a physical abstraction for value, and we abstract away currency in accounts.
		Negative numbers find use there, when we represent owing someone something as having a negative balance.
		Multiplication builds on addition, and allows you to add multiple times at once, or even add multiple negative times.
		The list goes on.
		Reality doesn&apos;t magically know about mathematical rules.
		Instead, we make up mathematical rules to use as tools for understanding our world and processing the data in it.
		If reality were different than it is now, maths would be different as well; mathematics are only our description and interpretation of the world.
	</p>
	<p>
		The article also talks about how there are representations for games we&apos;ve made up, and those games don&apos;t resemble the actual world.
		It seems to be saying that the universe is bound by maths, but maths aren&apos;t bound by the universe.
		Once we have our rules that can describe the system we live in though, we can use those same rules to describe things we made up, such as games.
		Humans are the most bizarre animals on the planet, and we don&apos;t always direct our focus at reality or even things that could conceivably fit into our reality.
		We make stuff up, but it&apos;s not useful to have a separate language and separate formalisms for describing real versus imaginary things.
	</p>
	<p>
		Once the article got into actually explaining functional programming, it had much more reasonable things to say.
		For example, unit testing in functional programs becomes much easier, because you don&apos;t have to worry about side effects and calling order.
		All you need to worry about is inputs and outputs.
		Deadlocks and race conditions are also non-existant in functional programming, though personally, I think this is another result of the lack of side effects in functions.
		How can you have a race condition when no data is getting modified?
		You just can&apos;t.
	</p>
	<p>
		The book touched upon the Windows and UNIX restart models for updating.
		On Windows, you restart the whole system.
		Or, you did in the past.
		I&apos;m not a Windows user, so I&apos;m not quite sure about this, but I think Modern Windows allows at least some updates without full system restarts now.
		That said, the system is still rather unstable, and needs to be restarted every month or so just to get everything back into alignment.
		We in the Linux world don&apos;t put up with that kind of nonsense.
		Linux follows the UNIX model, and restarts only the components being updated, as well as components that depend on the ones being updated.
		Components are shut down, replaced, then started back up.
		Linux systems tend to run for years at a time without degradation of performance.
		We pretty much don&apos;t need to restart the whole system except in case of kernel update, as the kernel is so important that the rest of the system relies on it and can&apos;t continue while it&apos;s down.
		But some Linux users even have hot-patching available for the kernel.
		I forget the details, but they don&apos;t have to restart their machines even for kernel updates, which is good for servers that need to be online at all times.
		Personally, I haven&apos;t made the effort to get that sort of thing set up because my distribution has such infrequent kernel updates.
		As far as I can remember, I don&apos;t think I&apos;ve encountered a kernel update except when upgrading to a new release of the system, where pretty much the entire set of software sees a major update, and I prefer to shut the system down to perform a clean installation at that time anyway.
	</p>
	<p>
		The article&apos;s take on currying seemed way different than that described by the textbook.
		The textbook described it as returning functions as return values and calling those functions with new input values.
		The article instead described it as being basically the adaptor pattern, where a wrapper function is created to translate an argument set to work with a different function&apos;s parameters.
		I&apos;m guessing it&apos;s the textbook that&apos;s correct, and that the article is just explaining the concept very poorly.
	</p>
	<p>
		I get the feeling infinitely-long the data structures mentioned by the article aren&apos;t much different than generators in imperative languages.
		The example used was Fibonacci numbers, and you can&apos;t just jump to an arbitrary Fibonacci number and compute it.
		To compute a Fibonacci number, you must compute all the numbers that came before it, just like with a generator.
		A generator too can go on as long as need be, with no end.
		When the user stops asking for the next output, no more output is given.
	</p>
	<p>
		The concept of &quot;continuations&quot; seems to just be a subset of higher-order functions.
		Basically, a callback is passed to the function, and that callback called with the would-be return value of the main part of the function.
		The function then returns the return value of the callback.
		It looks like they further have implications with execution order, though I&apos;m not entirely sure such implications couldn&apos;t be enforced even with just a basic higher-order function.
	</p>
</section>
		<hr/>
		<p>
			Copyright © 2018 Alex Yst;
			You may modify and/or redistribute this document under the terms of the <a rel="license" href="/license/gpl-3.0-standalone.xhtml"><abbr title="GNU&apos;s Not Unix">GNU</abbr> <abbr title="General Public License version Three or later">GPLv3+</abbr></a>.
			If for some reason you would prefer to modify and/or distribute this document under other free copyleft terms, please ask me via email.
			My address is in the source comments near the top of this document.
			This license also applies to embedded content such as images.
			For more information on that, see <a href="/en/a/licensing.xhtml">licensing</a>.
		</p>
		<p>
			<abbr title="World Wide Web Consortium">W3C</abbr> standards are important.
			This document conforms to the <a href="https://validator.w3.org./nu/?doc=https%3A%2F%2Fy.st.%2F%5BTEMP%5D%2F%5BTEMP%5D.xhtml"><abbr title="Extensible Hypertext Markup Language">XHTML</abbr> 5.2</a> specification and uses style sheets that conform to the <a href="http://jigsaw.w3.org./css-validator/validator?uri=https%3A%2F%2Fy.st.%2F%5BTEMP%5D%2F%5BTEMP%5D.xhtml"><abbr title="Cascading Style Sheets">CSS</abbr>3</a> specification.
		</p>
	</body>
</html>

