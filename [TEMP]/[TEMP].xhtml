<?xml version="1.0" encoding="utf-8"?>
<!--
                                                                                     
 h       t     t                ::       /     /                     t             / 
 h       t     t                ::      //    //                     t            // 
 h     ttttt ttttt ppppp sssss         //    //  y   y       sssss ttttt         //  
 hhhh    t     t   p   p s            //    //   y   y       s       t          //   
 h  hh   t     t   ppppp sssss       //    //    yyyyy       sssss   t         //    
 h   h   t     t   p         s  ::   /     /         y  ..       s   t    ..   /     
 h   h   t     t   p     sssss  ::   /     /     yyyyy  ..   sssss   t    ..   /     
                                                                                     
	<https://y.st./>
	Copyright Â© 2019 Alex Yst <mailto:copyright@y.st>

	This program is free software: you can redistribute it and/or modify
	it under the terms of the GNU General Public License as published by
	the Free Software Foundation, either version 3 of the License, or
	(at your option) any later version.

	This program is distributed in the hope that it will be useful,
	but WITHOUT ANY WARRANTY; without even the implied warranty of
	MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
	GNU General Public License for more details.

	You should have received a copy of the GNU General Public License
	along with this program. If not, see <https://www.gnu.org./licenses/>.
-->
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
	<head>
		<base href="https://y.st./[TEMP]/[TEMP].xhtml"/>
		<title>Learning Journal &lt;https://y.st./[TEMP]/[TEMP].xhtml&gt;</title>
		<link rel="icon" type="image/png" href="/link/CC_BY-SA_4.0/y.st./icon.png"/>
		<link rel="stylesheet" type="text/css" href="/link/main.css"/>
		<script type="text/javascript" src="/script/javascript.js"/>
		<meta name="viewport" content="width=device-width"/>
	</head>
	<body>
<nav>
</nav>
		<header>
			<h1>Learning Journal</h1>
			<p>CS 4406: Computer Graphics</p>
		</header>
<section id="Unit1">
	<h2>Unit 1</h2>
	<p>
		The reading assignment for the week was as follows:
	</p>
	<ul>
		<li>
			<a href="https://math.hws.edu/graphicsbook/c1/index.html">Introduction to Computer Graphics, Chapter 1 -- Introduction</a>
		</li>
		<li>
			<a href="https://math.hws.edu/graphicsbook/c5/index.html">Introduction to Computer Graphics, Chapter 5 -- Three.js: A 3D Scene Graph API</a>
		</li>
		<li>
			<a href="https://people.csail.mit.edu/fredo/Depiction/1_Introduction/reviewGraphics.pdf">A Short Review of Computer Graphics.PDF - reviewGraphics.pdf</a>
		</li>
		<li>
			<a href="https://www.cs.csustan.edu/~rsc/NSF/Notes.pdf">Notes.pdf</a>: Chapter 0 and Chapter 1
		</li>
	</ul>
	<p>
		Seeing that the Introduction to Computer Graphics textbook focusses on <abbr title="three-dimensional">3D</abbr> graphics piqued my interest.
		When I signed up for this course, I was hoping to learn how to better create <abbr title="two-dimensional">2D</abbr> graphics, a skill I&apos;d use on my website, for creating game graphics, and perhaps more.
		Working with <abbr title="three-dimensional">3D</abbr> graphics should be pretty interesting though.
		In the end, as the textbook says, the end result of <abbr title="three-dimensional">3D</abbr> graphics editing is a <abbr title="two-dimensional">2D</abbr> image.
		I didn&apos;t think I could use that sort of thing much in the type of game I need images for, but I should be able to get even better-looking Web graphics using <abbr title="three-dimensional">3D</abbr> methods.
		As I read on, I found we were discussing terminology I&apos;d seen in Blender when trying to create a <abbr title="three-dimensional">3D</abbr> model though.
		I&apos;m starting to get into some basic modelling in the game I work with, so what we learn in this course may help with my game graphics after all, just not the part I&apos;d thought about as graphics.
	</p>
	<p>
		The term &quot;frame buffer&quot; was new to me, but the concept wasn&apos;t.
		I was already aware that the colour values for the image on-screen was stored in some block of memory, and that changes made to that block of memory cause changes on the monitor without need to recalculate every single pixel.
		You can even see side effects of that system when things go wrong.
		For example, Firefox locks up on me sometimes.
		When it does, all Firefox windows stop calculating values for their pixels (aside from the pixels of the window border, which are handled by the window manager instead of Firefox itself), and these windows begin to capture the output of whatever was in their place previously.
		For example, if I minimise the window to reveal the windows behind it, then unminimise the malfunctioning Firefox window, the window boarder gets drawn, but the inner pixels of the window don&apos;t change, so I see a still frame of the windows that were behind the Firefox window before I brought it back up.
		Or I can drag a smaller window across the malfunctioning Firefox window and it&apos;ll leave a trail.
		All of this is because Firefox isn&apos;t updating the values in the frame buffer that it&apos;s responsible for, so those valuse remain at whatever another process last set them to.
	</p>
	<p>
		I was also aware of the term &quot;raster graphics&quot;, and know the difference between raster graphics and vector graphics, though I wasn&apos;t aware the term was a legacy term from the vacuum tube days.
		I&apos;ve never managed to figure out how to properly edit vector graphics in an editing program though.
		I manage to make use of the <abbr title="GNU Image Manipulation Program">GIMP</abbr> reasonably well, but I can&apos;t get the first shape into place in Inkscape.
		The best I&apos;ve managed to do is compose and edit the <abbr title="Extensible Markup Language">XML</abbr> of <abbr title="Scalable Vector Graphics">SVG</abbr> files in a code editor, using an image viewer to check the results so as to know what to adjust.
		It&apos;s a bit of a pain, actually.
		I was also already aware of the difference between lossy and lossless compression.
		I&apos;ve never liked the idea of lossy compression, but it does drastically cut down the file size of my photographs.
	</p>
	<p>
		On some level, I knew enough about <abbr title="three-dimensional">3D</abbr> graphics to know that they&apos;re created using <abbr title="two-dimensional">2D</abbr> shapes in a <abbr title="three-dimensional">3D</abbr> space.
		I knew they weren&apos;t generated using voxels, the <abbr title="three-dimensional">3D</abbr> equivalent of pixels.
		It hadn&apos;t occurred to me that this makes them very similar to vector graphics though.
		I have far too little experience with both vector graphics and <abbr title="three-dimensional">3D</abbr> graphics to have notice such a conclusion.
	</p>
	<p>
		The textbook explained that shaders are small programs sent to the <abbr title="graphics processing unit">GPU</abbr>, and that they don&apos;t always have anything to do with shading.
		That explains a lot in the game I work with.
		The developers added several features that they called shaders, but some of them can&apos;t even be remotely attached to the idea of shading.
		For example, one of them causes certain images to be displaced, and another couple case objects to move around visually.
		These features are probably implemented using <abbr title="graphics processing unit">GPU</abbr> shaders, so the developers are calling them shaders.
	</p>
	<p>
		I couldn&apos;t follow a lot of what the material on <code>three.js</code> was saying.
		It was a bit above my head.
		One thing I did understand though was how the faces in a <code>three.js</code> model don&apos;t specify the coordinates of their vertices directly, but instead specify where those vertices lie in the vertex array.
		That seems rather useful.
		I mean, it obviously means more overhead, but it also means more consistency.
		If a vertex gets moved, all faces using that vertex are essentially automatically updated.
		Or more accurately, the faces don&apos;t even <strong>*need*</strong> to be updated.
	</p>
	<p>
		The short review of computer graphics claimed purple to be at (255, 0, 255).
		That is definitely not purple.
		That&apos;s pure magenta, one of the eight most extreme colours; the others being black, white, red, blue, green, yellow, and cyan.
		There&apos;s some variance as to where people draw the lines between colour labels, but there&apos;s no mistaking what colours are at positions with all colour channels each set to either zero or two fifty-five.
		If you&apos;re working with graphics, you should know these eight basic colours.
	</p>
	<p>
		At first, I liked how the textbook is sensible enough to start at the beginning when numbering its chapters; which is to say, it starts at chapter zero instead of just skipping over it like most books, textbook or otherwise.
		However, it seems to only even call it chapter zero in some contexts, and not label it as even being a chapter in others, such as the table of context.
		It&apos;s sort of treating zero as less than a number.
		That&apos;s a let-down.
	</p>
	<p>
		The <abbr title="three-dimensional">3D</abbr> eye coordinate system seems bizarre.
		If I&apos;m understanding it correctly, instead of moving the camera, you move everything <strong>*except*</strong> the camera as a way to simulate camera movement.
		That seems rather complicated compared to just moving one object, the camera itself.
	</p>
	<p>
		The concept of parallel and perspective viewing volumes is interesting.
		With how complicated <abbr title="three-dimensional">3D</abbr> graphics rendering seems to be, I hadn&apos;t even considered something as simple as parallel viewing volume.
		I just sort of assumed that you always had to calculate trajectories from objects to the viewing point, which makes the image more realistic, but also more complicated to produce.
		It seems that&apos;s actually optional though, and can be done depending on what sort of effect you&apos;re going for.
	</p>
	<p>
		The section on converting coordinates from the eye space to the display space was enlightening.
		It hadn&apos;t occurred to me that the eye space uses floats instead of integers for its coordinate system.
		I should have realised though.
		I really don&apos;t know how to make much work in Blender yet, but I&apos;ve worked with it a little bit.
		I managed to make a cube and get it properly sized and textured for what I needed.
		That&apos;s the extent of what I&apos;ve done in Blender.
		However, that little bit should have been enough to show me that the coordinate system used was in floats.
		I mean, I had to get the cube&apos;s side length to <code>1.28</code> before it came out right, and that&apos;s clearly not an integer.
		The eye space is an intermediate step between the world space and the <abbr title="two-dimensional">2D</abbr> image, so there&apos;s no reason for it to jump immediately to integer coordinates.
		If it did, it really wouldn&apos;t be any different than the final <abbr title="two-dimensional">2D</abbr> image&apos;s coordinate space and could simply be skipped.
	</p>
	<p>
		If we&apos;re going to be working the way the textbook describes, we&apos;ve going to be defining callbacks to do our work instead of doing it directly.
		I&apos;m well familiar with using callbacks, but it&apos;s surprising that we&apos;d be handling graphics this way, if this course is at all what I thought it was before I registered.
		I thought we were generating graphics, such as <abbr title="Portable Network Graphic">PNG</abbr> files.
		If we&apos;re instead creating interactive scenes, that&apos;s an entirely different situation, though callbacks would certainly make the most sense in such an environment.
	</p>
	<p>
		I thought that transforming the world space to convert from world coordinates to eye coordinates would be very complex.
		The textbook did a great job of explaining just how simple the calculations are though.
		At least, when dealing with an orthographical view, these calculations are extremely simple.
		You just move the world using the reverse path in which you moved the eye, setting the eye back into the default position in the process (as moving the world moves the objects in the world, such as the eye itself).
		The world thus becomes eye-centric.
		Perspective views perform further calculations that are a little more complex.
		You basically divide a point&apos;s <var>x</var> and <var>y</var> values by their <var>z</var> value to find where on the <abbr title="two-dimensional">2D</abbr> image space they project to.
		It seems simple except when I try to think about it too hard, at which point I start looking for some complex way to determine which otherwise-visible points are ignored because they&apos;re obscured by closer points.
		The obvious answer though is to render everything, and when it comes to projecting to the <abbr title="two-dimensional">2D</abbr> space, only write the colour of the relevant pixel according to the point with the <var>z</var> value closest to zero.
		It&apos;s a bit difficult for me to keep in mind that you can compare all the objects; you don&apos;t have to take a pixel and use it to calculate the one and only object it can see, then deal exclusively with that object.
		Rendering isn&apos;t a lookup operation.
	</p>
	<p>
		I recently learned the term &quot;z-fighting&quot;, and immediately recognised the visual effect from a particular game I play sometimes.
		However, I&apos;d never given much thought as to exactly why it occurs.
		The textbook gave an excellent explanation though.
		Items occupying the same space have the same <var>z</var> value, so when <code>z</code> values are compared to see if the current object is the closest thing to the camera, the points on both objects, when the object is drawn, register as the closest and are able to draw to the pixel.
		Next time the other equally-close object is drawn, it overwrites the value set by the previously-writing object, and the value of the pixel flips back and forth.
		Very interesting.
		The textbook framed it as a rounding issue when converting from continuous numbers to integers, but I don&apos;t think that&apos;s actually all that relevant to the issue.
		First, two objects could share exactly the same continuous number coordinates and that same effect would occur, with or without rounding.
		And secondly, the computer representation of a float has precision limitations too; there&apos;s not just integer size limits.
		So even if numbers remained as floats and were used that way, z-fighting would still occur.
		The rounding certainly makes z-fighting more likely to occur though.
	</p>
	<p>
		Reading through the assignment notes for the week, they said that we&apos;d be working with <code>three.js</code> because it doesn&apos;t require C++ and because it uses JavaScript and JavaScript is a language we&apos;ve worked with previously in this school&apos;s computer science curriculum.
		Um.
		What?
		I don&apos;t remember writing a <strong>*single*</strong> line of JavaScript at this school!
		I <a href="https://y.st./en/coursework/">archive</a> all my coursework though, so I went back and checked for the presence of JavaScript.
		Sure enough, there wasn&apos;t a single JavaScript file and not a single assignment that related to JavaScript in any way.
		And for context, I&apos;ve taken <strong>*both*</strong> Web programming courses already.
		They covered <abbr title="PHP: Hypertext Preprocessor">PHP</abbr>, <abbr title="Extensible Markup Language">XML</abbr>, <abbr title="Cascading Style Sheets">CSS</abbr>, and <abbr title="Hypertext Markup Language">HTML</abbr>, but not JavaScript.
		JavaScript had <strong>*not*</strong> been a part of this school&apos;s curriculum!
		I think we discussed JavaScript one week in one of the courses, but we definitely didn&apos;t learn it.
		After writing my thoughts on that though, I went back to reading the notes, and noticed the exact wording used.
		JavaScript was one of the languages &quot;introduced&quot;, not one of the languages &quot;learned&quot;.
		Okay, the notes had me there.
		We&apos;d definitely had JavaScript introduced to us in the form of telling us it existed, we just weren&apos;t taught how to actually use it.
		Just like we were introduced to the fact that Perl is a language, but again, not actually taught to make use of that language.
	</p>
	<p>
		I know some JavaScript from my own experiments outside the school, though JavaScript is a language I&apos;m very weak in.
		With access to the Internet at my fingertips though, I&apos;ll be able to look up the functions I need to make it through the class, I think.
		As for C++, I&apos;ve been hoping that C++ would be taught in a course I&apos;ll take in the future, though this seems to confirm that it won&apos;t be.
		It doesn&apos;t seem to be taught at this school, which is disappointing.
	</p>
	<p>
		The notes also say to get the code for the assignment from the &quot;Development Environment link in the course page&quot;.
		No such link exists on the course page.
		I checked the syllabus too, and it&apos;s not there.
		I ended up giving up and checking the assignment page, which I&apos;d put my reading of on hold to read the notes, as the assignment page told me to read the notes.
		The code was there, as it should be, and not through some separate &quot;Development Environment&quot; link.
	</p>
	<p>
		This seems to be the harder of my two courses this term.
		Some of the material seems to be above my head.
		Hopefully I can pull it together and figure out the material.
		I&apos;d love to learn how to generate and render <abbr title="three-dimensional">3D</abbr> scenes like this.
		Honestly, I don&apos;t see this stuff applying to a job I&apos;d be interested in, but off the clock, this seems like I could do a lot of things with it.
		It might even help me contribute to a project I like called Minetest.
		It&apos;s a <abbr title="three-dimensional">3D</abbr> game in which the world is made up of giant cubes.
		It uses the Irrlicht engine, mentioned briefly in one of the tables presented in the reading material, and relies on code contributions from the community for its continued development.
		I&apos;d love to be a part of what makes Minetest so great.
		The textbook also mentioned Blender.
		If I could even learn to use Blender properly, I could improve the plug-ins I write for Minetest, even if I didn&apos;t have the skills to contribute to the Minetest engine itself.
		In any case, I feel like there&apos;s a lot I could get from this course if I can wrap my head around the material better.
	</p>
</section>
<section id="Unit2">
	<h2>Unit 2</h2>
	<p>
		This week&apos;s reading assignment was <a href="https://www.cs.csustan.edu/~rsc/NSF/Notes.pdf">Notes.pdf</a>, chapters two and three.
	</p>
	<p>
		The grading rubric for this week was bizarre.
		It had us grade last week&apos;s assignment based on the completely-different criteria laid out by this week&apos;s assignment instructions.
		The obvious choice was to give everyone a zero, as the criteria didn&apos;t fit what anyone actually did.
		It&apos;s a good thing this assignment isn&apos;t worth points.
		Or at least, it claimed last week not to be worth points, so hopefully, it isn&apos;t.
		However, it seems rather unfair to grade that way, so I instead gave everyone full credit with my notes explaining that the rubric given didn&apos;t match the assignment.
	</p>
	<p>
		The first student submitted the example code from the assignment.
		I considered doing this, but there were a couple reasons I didn&apos;t.
		One of those reasons being that we didn&apos;t actually modify the code in any way, which means that code wasn&apos;t our work; our work was to read the notes and try to understand them.
		Submitting the code wouldn&apos;t be an accurate representation of my efforts.
		The second student provided a screenshot of what they saw, which was interesting.
		It allowed me to verify that I wasn&apos;t the only one not seeing something similar to what the notes for the assignment showed in its included screenshot.
		Like what I saw, this student saw no distinction between the faces of the pseudo-sphere.
		The third student did what I did, and submitted their thoughts on the assignment.
		They kept it much more brief than I did; as I&apos;m sure you can see here, I tend to be very verbose.
		I tend to learn best when I&apos;m writing about what I&apos;m studying, and if I don&apos;t write enough, I tend not to learn as much.
	</p>
	<p>
		Most of this week&apos;s reading material was much easier to understand than last week&apos;s.
		A lot of it was stuff I already knew from mast experience, such as how transformations work on shapes in a coordinate space.
		One thing about the explanation of transformation order confused me.
		In the example <abbr title="application programming interface">API</abbr> calls, three transformations are applied and then the geometry is set up.
		How can the geometry be set up last?
		Doesn&apos;t there need to be the base geometry before any transformations are applied so the transformations can have something to be applied to?
		Also, the textbook said that the transformations are applied in reverse order, because they&apos;re applied in terms of which transformation is &quot;nearest&quot; to the geometry.
		What I understood that to mean is that because the <abbr title="application programming interface">API</abbr> call associated with the final transformation was closest to the <abbr title="application programming interface">API</abbr> call setting up the geometry, it was the &quot;closest&quot; transformation to the geometry.
		That was the only thing that made sense in the context.
		Except that it didn&apos;t make sense.
		An <abbr title="application programming interface">API</abbr> call can&apos;t be run on data that doesn&apos;t exist, then retroactively get applied when the data becomes available.
		That&apos;s not how operating on data works.
		You could set up a stack to push the operations onto so you could make it appear that that&apos;s the way operating on data works, and doing so would result in that reverse order behaviour, but that&apos;d be unnecessary overhead.
		I could only assume that I&apos;ve completely misunderstood what&apos;s going on.
		Later on, the textbook specifically said that transformations are applied in the order &quot;last-specified, first-applied&quot;.
		This pretty much means the stack theory isn&apos;t too far off.
		However, it still seems bizarre that you wouldn&apos;t code in the order you need things done.
		For some reason, the graphics <abbr title="application programming interface">API</abbr> is having us code backwards.
		Is there any good reason for this?
		I have to assume that someone thought there was, and that someone knows a lot more about graphic rendering than I do.
		And then the textbook went back to calling it applying the transformation &quot;closest&quot; to the geometry again.
		Which still doesn&apos;t make any sense.
		<abbr title="last-in, first-out">LIFO</abbr> is something I can understand and work with though.
	</p>
	<p>
		Soon, the textbook started comparing the transformations to a stack as well.
		But the context at that point was that previously-defined transformations affect later geometry.
		If you define a bunch of transformations, then define a cube to make use of those transformations, the cube will be transformed because everything is done in reverse order for whatever reason.
		But then, when you define an unrelated sphere, all the same transformations apply to the sphere as well.
		You don&apos;t define transformations that will apply to an object you need transformed.
		Oh no.
		That would actually be easy to work with, and we can&apos;t have that.
		Instead, you need to first transform the entire, empty scene, then throw objects into it ant they all get transformed the same way.
		Not going to work for you?
		In that case, you need to define the transformations for your first object, define the first object, undo the transformations, add new ones that apply to your second object, then define the second object.
		What a complete mess.
		What in the world were the people that came up with this system thinking!?
		I have to keep reminding myself that they must have known something I&apos;m not thinking of.
		I mean, there&apos;s obviously a lot they know that I don&apos;t, but there must be something huge I&apos;m missing if this is actually somehow a good system.
	</p>
	<p>
		The explanation as to why meshes are made out of triangles was enlightening.
		I&apos;d assumed it was a limitation of the graphics library.
		I figured it was easier for the developers of the library to disallow polygons with an arbitrary number of vertices.
		You can build other polygons out of triangles, so allowing only triangles is &quot;good enough&quot;.
		That&apos;s not actually the case at all though.
		If you have three or fewer points, there will always be at least one plane that they all lie on.
		If you add a fourth point though, it can be off-plane from the other planes, so the surface of the face it represents would be undefined.
		Limiting us to work with triangle prevents such undefined surfaces.
		There are ways around that issue without limiting sides.
		For example, you could change the format for specifying polygons, so the first parameter is the equation of a plane and the second parameter is the list of vertices in the order they connect.
		The vertices would be transformed from the <abbr title="two-dimensional">2D</abbr> space of the plane to the <abbr title="three-dimensional">3D</abbr> space of the world by the graphics library, but because they were originally defined in a <abbr title="two-dimensional">2D</abbr> space, you&apos;d have absolute certainty that they all lied on the same plain.
		What a nightmare though, both for the developers of the library and users of it!
		It would really complicate the format and make it harder to work with.
		Limiting faces to being triangular makes life easier for everyone involved.
	</p>
	<p>
		I wasn&apos;t clear on what normals are at first, either.
		The textbook says you might need them if your object needs to face a certain direction, but didn&apos;t tell what they actually are.
		From the context, I was guessing that they might be some angular offset used to shift all faces of a model by, but I really didn&apos;t know for sure.
		Much, <strong>*much*</strong> later, as in in another chapter even, the textbook actually told us what normals are.
		A normal is a vector that&apos;s perpendicular to its respective face.
		Was that so hard?
		Using terminology and then only later actually explaining it really makes it hard to learn.
		I spent much of the reading confused as to what normals even were, so I had no way to process much of what was said about them.
		I&apos;d go back and reread the chapters again, but there really isn&apos;t time in the week for that, so that information is just lost to me and I&apos;m probably going to end up struggling later.
		Some of what I need will likely be found through specific Web searches, but some of it likely won&apos;t be.
		And even if it all was all easy to find online, there&apos;s really no benefit in the textbook using unknown terminology long before explaining it.
		It didn&apos;t even take long to explain it when the book finally got to it (a single sentence, really), so it could easily be fit in before the first usage of the term.
	</p>
	<p>
		The textbook brought up another good reason graphics <abbr title="application programming interface">API</abbr>s allow you to model things in their own space, then place them in the scene.
		It already mentioned object reuse last week.
		That reason alone is enough to warrant <abbr title="application programming interface">API</abbr> options for the task.
		But then this week, it makes the claim that it&apos;s difficult to model things in the world at their intended locations.
		Boo hoo.
		You just need better graphics-editing tools, if you can&apos;t pull that off easily.
		But then it brings up that things need to move through the scene, if the work is animated.
		Modelling would be a nightmare if you had to adjust the coordinates of each vertex of the image every time the object as a whole moved.
		That&apos;s definitely a second good reason for providing the tools needed to model things in their own space.
	</p>
	<p>
		The textbook makes a great point about why models should usually be centred about the origin.
		When the section began, I actually started thinking about off-centre objects and how being off-centre would change the effects of scaling.
		I concluded that everything would be fine, as the distances between the surfaces would be the same, so the object would scale to the same.
		Mostly, I was write.
		However, the book pointed out that objects are scaled about the origin, so if an object is off-centre, scaling it will have the side effect of moving it through space.
		Well, I mean, the object is still technically located in the same spot, by the computer&apos;s perspective..
		However, the the place it <strong>*renders*</strong> will change.
		The computer sees the object as existing at the finite point we moved the origin of the object to, but what we perceive as the object gets moved out of place.
		I can see some use cases for off-centre origins though.
		For example, if a tree&apos;s origin was at the base of the model, scaling it wouldn&apos;t cause the tree&apos;s trunk to plunge into the ground (or jump up into the air) and need to be placed back on solid ground again.
		The textbook also mentions that rotating objects whose centre is at the origin is also easier because the object likewise doesn&apos;t move through space in unexpected ways.
		For most models, centring then at the origin really seems like the best option.
	</p>
	<p>
		The textbook explained why in most cases, you should scale, rotate, then translate an object.
		Other orders have less-intuitive and harder-to-work-with effects.
		If we pay attention here though, there&apos;s more we can pick up from the example of the distorted box: transformations are applied not based on an object&apos;s own coordinate space, but based on the world space the object resides in.
		We shouldn&apos;t, therefore, think about translations in the model&apos;s own space when we&apos;re trying to transform them in the world.
		We should think of them similarly to primitives we might have used in the world directly.
	</p>
	<p>
		The textbook brought up that because scaling is multiplication, reversing it is division.
		With that in mind, it continued, you can&apos;t undo a scale to zero.
		Huh.
		Right.
		Division by zero errors.
		I admit I hadn&apos;t thought of those, but not because I simply didn&apos;t think about division by zero.
		Instead, because I hadn&apos;t thought about <strong>*multiplication*</strong> by zero!
		In the real world, if you multiply something by zero, you have nothing.
		Nothing exists in two dimensions, so multiplying the width of something would result in having no object and no volume.
		In a computer representation though, <strong>*everything*</strong> is in two dimensions.
		The faces can be arranged at arbitrary angles in <abbr title="three-dimensional">3D</abbr>, but the faces themselves are two-dimensional, and the faces are the only things that actually exist in the scene.
		Scaling a dimension by zero simply crunches the face back into a two-dimensional orientation parallel with two of the axes; it doesn&apos;t make it disappear altogether.
		You could even scale all three dimensions by zero and get a face with all vertices existing at the same point in space.
	</p>
	<p>
		Later, the textbook finally explained what it meant by transformations being &quot;closer&quot; to the geometry.
		It meant that these transformations were nested deeper in a scene graph, closer to the node representing the relevant geometry.
		When the textbook first mentioned transformations being &quot;closer&quot; to the geometry, it provided no such explanation of what it was even talking about.
	</p>
	<p>
		The textbook mentions that you can&apos;t create a cube out of a single quad strip.
		This is very true, but I think it&apos;s worth briefly taking a look at why.
		Six connected squares could easily be folded into a cube, depending on how they are connected.
		In fact, each of those squares could be connected to no more than two other squares, creating a continual path of squares.
		Take four squares in a row, then attach a square above one end and one below the other.
		However, quad strips can&apos;t be defined that way.
		In a quad strip, the two vertices shared between a pair of adjacent squares can&apos;t be shared by any other square.
		In other words, you can&apos;t define three squares in the quad strip to share a single vertex, and a continual strip of squared that could fold to become the faces of a cube would have to have at least two vertices that were each shared by three squares.
	</p>
	<p>
		The textbook at first said that display lists were named unsigned numbers.
		Of course, I would try to begin naming lists from zero.
		I&apos;m a programmer.
		Programmers realise that zero is a valid number, and don&apos;t skip it when indexing or naming.
		A bit later though, the textbook reiterates, but this time says you need to use unsigned, <strong>*non-zero*</strong> integers.
		That makes a huge difference.
		I could have easily gotten stuck trying to debug for hours if using zero silently fails and the inability to use zero wasn&apos;t mentioned.
		Of course, I don&apos;t know if it silently fails or not.
		Maybe it halts the script and spews visible error messages telling you exactly what the problem is.
		We can always be hopeful that that&apos;s the sort of behaviour a library will have, so you&apos;re not left trying to debug simple problems without any clue as to what the problems actually <strong>*are*</strong>.
	</p>
	<p>
		The assignment for the week was a major headache.
		Whenever I try to run the code given to me by the university in my Web browser directly, no graphics are rendered and the Firebug console gives me the error message &quot;Access to restricted URI denied.&quot;.
		So I can&apos;t see what I&apos;m doing.
		Last week, the code ran just fine on the JSbin website.
		This week, it doesn&apos;t render there, either.
		I was going to have to work blind.
	</p>
	<p>
		I tried.
		I really tried.
		But I just can&apos;t work if I can&apos;t see.
		You should see me when I&apos;m coding.
		I often write small fragments at a time, then running my code to see if it did what I expected.
		Even for the most-basic stuff.
		And I often botch things pretty badly.
		But because I can <strong>*see*</strong> what&apos;s going on, I can correct it.
		I may not even understand what goes wrong right away, but I just keep adding output to the problem area until I find what output isn&apos;t as expected, and can fix it.
		But I can&apos;t do that if I can&apos;t see.
		And I have no idea what I&apos;m doing when it comes to WebGL, so I can&apos;t even assume I&apos;m coding things right and move on.
		I can&apos;t work blind.
	</p>
	<p>
		There was no time to ask for help.
		I&apos;d taken too long to read the textbook this week.
		The material is difficult to get through, and takes me forever.
		I needed to get my assignment done quickly in the remaining time, and if I waited for a response to my queries, the deadline would pass.
		I decided to try to figure out what script was misbehaving and causing that error message.
		I deleted all references to scripts aside from the main Firebug script, hoping to add them back, one by one, until Firebug complained again.
		However, Firebug didn&apos;t quit complaining, so it couldn&apos;t start again.
		The main Firebug script was trying to access some forbidden <abbr title="Uniform Resource Identifier">URI</abbr>!
		I don&apos;t know what the problem is, but Firebug just isn&apos;t going to run locally, it seems, and I just don&apos;t get it.
	</p>
	<p>
		Next, I tried to figure out what was so magical about last week&apos;s code.
		Why did it run in JSbin when this week&apos;s code doesn&apos;t?
		The first step was to see if I could inject the code for the yellow triangle from the example that wouldn&apos;t run into the sphere code from last week that did run.
		I couldn&apos;t see the triangle.
		I assumed it was inside the sphere, or something.
		So I found the line that added the sphere to the scene and commented it out.
		The sphere was still created in the code, it just wasn&apos;t added to the scene and rendered.
		Success!
		I had a spinning, yellow triangle.
		Is the example code this week just broken?
		Perhaps it&apos;s just broken.
		But why?
		So I started comparing the two files to see what might be the problem.
	</p>
	<p>
		The first thing I noticed were the line endings.
		The rendering code used <code>\n</code>, while the dysfunctional code was using <code>\r\n</code>.
		That shouldn&apos;t cause a problem in the rendering, but it was distracting, as it made every line in the editor change as I flipped between the two files.
		I couldn&apos;t see what had really changed.
		So I converted all the line endings to <code>\n</code>.
		Next, I saw the dysfunctional code didn&apos;t have a <code>&lt;title/&gt;</code> element.
		That made the dysfunctional code technically invalid, but I didn&apos;t think JSbin would care enough to disable the output over it.
		And actually, the Firebug console was showing up anyway, proving that JSbin was in fact trying to properly set the JavaScript to run.
		Next, I saw ... the script tags were different.
		Last week&apos;s included JavaScript files were fewer in number and mostly hosted by my nemesis, CloudFlare.
		Actually, I&apos;m surprised those files actually work because of who&apos;s hosting them.
		This week&apos;s JavaScript files were greater in number and mostly hosted by the university.
		Could that be the problem?
		I swapped out the scripts, and found I was working with the wrong file: the one that wasn&apos;t supposed to render anything.
		The spinning triangle example, while still unable to render, actually does have a proper <code>&lt;title/&gt;</code> element.
		Also, most of the scripts are instead hosted on some website about healthy living that no longer exists, and thus can&apos;t serve the scripts.
		Bingo.
		I&apos;d found the problem.
		But also, why do all three of these files have different base environment scripts?
		I mean, two of these are even supposed to be the same assignment.
		Why would they have different environments?
		And on closer inspection, the two files that seemed to have a <code>&lt;title/&gt;</code> element actually have <strong>*two*</strong> <code>&lt;title/&gt;</code> elements, which is invalid, while the file I thought didn&apos;t have any actually had one.
		Whoever wrote these files didn&apos;t know what they were doing.
	</p>
	<p>
		With the reason for the yellow triangle file not working seemingly found, and having nothing to do with the template file, I tried injecting the yellow triangle into the template file.
		Still no rendering.
		But when I next completely replaced the sphere&apos;s geometry in last week&apos;s code, the yellow triangle rendered just fine.
		Next, I tried replacing the linked scripts in both files that wouldn&apos;t render with links to the scripts from the working file.
		I thought for sure that that&apos;d work, but it didn&apos;t.
		It was time to look for more file differences.
		This time, I found that the broken code was using the <code>Detector.webgl</code> property to conditionally choose a renderer, while the working code did not.
		So I deleted the conditional.
		That didn&apos;t fix anything.
		The hight and width of the window were set differently as well, and while I didn&apos;t think it would change anything, I tried setting them according to the working file&apos;s values just to rule that out.
		Sure enough, no effect.
	</p>
	<p>
		Next, I came across the line that statically set the renderer in the working file.
		That was missing from the broken code, because the broken code was using the conditional statement instead to set the renderer.
		Copying over the renderer-defining line, I finally got the example code to work.
		Except ... the triangle was bigger now, and no longer spun.
		The spinning and apparently some zooming out had come from elsewhere in the sphere code.
		With the rendering now working, it was time to undo all of my changes, then redo the renderer fix, to reset me back to a clean starting place.
		The code went back to not working, but after replacing the <code>&lt;script/&gt;</code> tags again, everything went back to rendering properly.
		Next, I tried only fixing the renderer in the template file we were given, as it used scripts from the university&apos;s website, but that wouldn&apos;t render either, and I needed to swap out the <code>&lt;script/&gt;</code> tags there, too.
		It seems the university is no longer hosting the required scripts, or something.
		Long story short, the files this week both use code that doesn&apos;t work and rely on scripts that aren&apos;t there any more.
	</p>
	<p>
		After debugging all that, I started trying to build my solution, and noticed that my spell-checker had detected a misspelled word in a comment, so I read the comment while I was fixing that.
		The comment told me to disable the detector.
		If it needed to be disabled, why was it included?
		But also, disabling that hadn&apos;t been the only thing needed.
		I&apos;d needed to replace the <code>&lt;script/&gt;</code> tags.
	</p>
	<p>
		With my template now working, I copied over the triangle code, then painted the triangle blue.
		It disappeared though!
		I thought I was dealing with another bug, and after being unable to find it, I just copied over the triangle again, overwriting the painted one.
		It rendered.
		So I painted it blue again.
		And it disappeared!
		I spotted an ambient light that had been added to the scene.
		It had no blue component.
		If I&apos;d chosen to make the triangle red, as the assignment allows our rotating square to be either red or blue, it would have no doubt showed up in the light, but a blue object absorbs red and green light, reflecting only blue.
		And there was no blue to reflect.
		I changed the ambient light to output white light, and sure enough, the triangle regained visibility.
		Perfect.
	</p>
	<p>
		Next, I tried adding another vertex to the triangle.
		It&apos;d be horribly unsquare, but I needed to see if I could simply add vertices.
		It seems I could not.
		I added two more to see if it&apos;d add a second triangle.
		When I saw it did nothing, I was about to set the camera to zoom out a bit and see if the vertices were off-screen, I remembered I hadn&apos;t added edges or faces.
		Of course they weren&apos;t rendering.
		From here, I decided to just scrap the triangle and try out a triangle strip.
		I couldn&apos;t figure out how to do that though.
		After some unsuccessful Web searching, I gave up trying to do this the efficient way and instead just drew two, unconnected triangles that happen to share two vertices.
		I defined four vertices, each with an <var>x</var> and <var>y</var> value of <code>-5</code> or <code>5</code>, then added two faces, one of which used all the vertices with at least one negative coordinate and one that used all vertices with at least one positive coordinate.
		I guess I didn&apos;t need to specify the edges manually?
		Anyway, a blue square now sat near the middle of the window.
		The square was a bit low to the bottom though.
		I&apos;m not sure why that is.
		Looking at the code that sets the camera&apos;s position, it should be pulled back along the <var>z</var> axis, but centred on the <var>x</var> and <var>y</var> axes.
		I could easily adjust the square&apos;s coordinates to compensate, but without understanding the reason the camera was off-centre, I was reluctant to compensate for it like that.
		The assignment never said the square had to be centred, so I left it.
	</p>
	<p>
		Next, it was time to rotate the scene.
		I wasn&apos;t sure how to do that, so I took a look at the code from last week.
		It was using the <code>autoRotate</code> method of an object returned from <code>THREE.OrbitControls</code>.
		It wasn&apos;t actually defining the rotation itself.
		Is that what I was supposed to do too?
		I undid all my changes to the template document to take a look at the original <code>&lt;script/&gt;</code> tags.
		Sure enough, a file with a name indicating that it was specifically for defining these controls was included.
		I was supposed to use the orbit controls.
		But also, i thought I realised what was going on with the university&apos;s JavaScript files.
		They were probably behind the school&apos;s login wall!
		I did a quick check, and saw that no, the university was actually just sending bizarre files instead of the JavaScript files.
		The one I looked at was mostly broken characters, but also mentioned OS X, something that looked like a Java package name Apple might produce given their domain name, and something about Google Chrome.
		None of that software is installed on my machine, so it wasn&apos;t some file reacting to my setup or something.
		It was just a broken or misnamed file.
	</p>
	<p>
		I copied over the orbiting code, figuring I&apos;d need to make changes before it&apos;d run, but tried it out anyway.
		It worked just fine.
		I&apos;d adjust it later anyway, but for now, I wanted to get a working sample in place.
		I definitely wanted to clean up the code before submission though, so all of this would be reworked a bit.
		The position of the square was really bothering me though.
		I looked through the code to see if I could find any code that looked like it offset the camera&apos;s view from its actual location somehow, and found that <var>VIEW_ANGLE</var> was set to <code>45</code>.
		That wasn&apos;t right.
		That should be <code>0</code> or <code>90</code>, I wasn&apos;t sure which, if we wanted to see the square head-on.
		That might be the cause of the offset.
		I set it to <code>90</code> and tried again.
		The result was much better.
		It was still a bit off, but it was within the range of what I&apos;d tolerate.
	</p>
	<p>
		Looking back at the instructions to see if I&apos;d missed anything besides the inclusion of comments, which I&apos;d add as I cleaned up the code, I saw that the assignment also wanted us to scale the object and move it elsewhere in space.
		How would the graders even know if the object had been scaled (verses defined at the shown size to begin with) or moved (as opposed to defined in the shown location to begin with)?
		These seemed like strange requirements.
		Additionally, the reading material this week told us how to scale in OpenGL, but we&apos;re instead working in WebGL.
		I tried looking up how to scale in WebGL, but it seems transformations in WebGL require shaders, which we haven&apos;t worked with.
		I mean, this is the first actual assignment.
		The textbook talked about shaders, but not how to produce them, and even if it had, it only talked about them in the context of OpenGL, not WebGL.
	</p>
	<p>
		I couldn&apos;t find any good tutorials on shaders, but from the scraps of information I was able to piece together from comparing what the tutorials I couldn&apos;t understand were doing, shaders seem to use very specific variable names for input.
		They don&apos;t even look like function arguments, but you basically use them as function arguments after declaring them.
		If they have the right name, WebGL will set those variables as if they were arguments being passed in.
		It&apos;s a confusing system, likely related to WebGL&apos;s (and OpenGL&apos;s) unintuitive choice to operate on global variables outside the shaders too, instead of using arguments and return values like a clean library does.
		From the code samples I could find, it seemed that <code>getWebGLContext()</code> needed to be called on the canvas to return some other object with properties we could use to initialise the shader with, but what canvas?
		I couldn&apos;t find any canvas objects in the code we were working with.
		The closes thing I could find was the scene object, but passing that into <code>getWebGLContext()</code> (even without doing anything with the returned object) prevented rendering, as did passing in anything I could think might work, including passing nothing.
		It was like the call to <code>getWebGLContext()</code> itself was breaking the rendering, and seemed to have nothing to do with the argument.
		I did notice that some of the setup code for the rotation was actually included in the boilerplate part of the template though, so I was able to delete my copy of it from the section we were supposed to edit.
	</p>
	<p>
		Eventually, I had to give up.
		We haven&apos;t covered how to use shaders yet, or if we did, I didn&apos;t understand it.
		And why would we even be jumping right into shaders in the first assignment, the one in which we were just figuring out how to even get geometry to render at all?
		It made no sense.
		Getting a shape of our choice in a colour of our choice and making it spin seemed like a good first assignment on its own.
		It seems like shaders should be given their own assignment, at the very least, not to mention that the assignment this week didn&apos;t even mention shaders.
		For that matter, it didn&apos;t even tell us to scale or translate the square, it only said that we&apos;de be graded on whether or not we&apos;d demonstrated how to scale and translate objects.
		There were no actual instructions telling us in where or why to move the square or why  or to what size to scale the square.
		Things just weren&apos;t adding up.
		Due to seeing this assignment&apos;s grading rubric early, as it was mistakenly used as the grading rubric for last week&apos;s work, I knew I&apos;d be penalised for not showing a translation or scaling, but I just couldn&apos;t figure out how to pull it off.
		I&apos;ll just have to accept the lower grade and try to do better next week.
	</p>
	<p>
		While cleaning up the code and commenting, I set the triangles to only render from one side.
		That allowed me to see that my triangles were actually facing opposite directions, so I reordered my vertices, then built new triangles using those vertices in a more-consistent manner.
		That got them to face the same way, after which I made them render on both sides again.
		I ended up removing the camera angle adjustment I&apos;d added at first too.
		It didn&apos;t seem to be necessary, or even useful, by the time I got to cleaning up that part of the code.
		I think the off-centre issue was because the console wasn&apos;t letting me scroll all the way to the bottom or something.
		When the console is completely minimised, the square seems centred just fine.
	</p>
</section>
<section id="Unit3">
	<h2>Unit 3</h2>
	<p>
		...
	</p>
</section>
<section id="Unit4">
	<h2>Unit 4</h2>
	<p>
		...
	</p>
</section>
<section id="Unit5">
	<h2>Unit 5</h2>
	<p>
		...
	</p>
</section>
<section id="Unit6">
	<h2>Unit 6</h2>
	<p>
		...
	</p>
</section>
<section id="Unit7">
	<h2>Unit 7</h2>
	<p>
		...
	</p>
</section>
<section id="Unit8">
	<h2>Unit 8</h2>
	<p>
		...
	</p>
</section>
		<hr/>
		<p>
			Copyright Â© 2019 Alex Yst;
			You may modify and/or redistribute this document under the terms of the <a rel="license" href="/license/gpl-3.0-standalone.xhtml"><abbr title="GNU&apos;s Not Unix">GNU</abbr> <abbr title="General Public License version Three or later">GPLv3+</abbr></a>.
			If for some reason you would prefer to modify and/or distribute this document under other free copyleft terms, please ask me via email.
			My address is in the source comments near the top of this document.
			This license also applies to embedded content such as images.
			For more information on that, see <a href="/en/a/licensing.xhtml">licensing</a>.
		</p>
		<p>
			<abbr title="World Wide Web Consortium">W3C</abbr> standards are important.
			This document conforms to the <a href="https://validator.w3.org./nu/?doc=https%3A%2F%2Fy.st.%2F%5BTEMP%5D%2F%5BTEMP%5D.xhtml"><abbr title="Extensible Hypertext Markup Language">XHTML</abbr> 5.2</a> specification and uses style sheets that conform to the <a href="http://jigsaw.w3.org./css-validator/validator?uri=https%3A%2F%2Fy.st.%2F%5BTEMP%5D%2F%5BTEMP%5D.xhtml"><abbr title="Cascading Style Sheets">CSS</abbr>3</a> specification.
		</p>
	</body>
</html>

